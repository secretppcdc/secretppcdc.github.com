{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qh6qToBNRro4"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    fileobj = open(file_path, 'r', encoding='utf-8')\n",
    "    samples = []\n",
    "    tokens = []\n",
    "    tags = []\n",
    "\n",
    "    for content in fileobj:\n",
    " \n",
    "        content = content.strip('\\n')\n",
    "\n",
    "        if content == '' or content == '\\t':\n",
    "            if len(tokens) != 0 :\n",
    "                samples.append((tokens, tags))\n",
    "                tokens = []\n",
    "                tags = []\n",
    "        else:\n",
    "            contents = content.split('\\t')\n",
    "            tokens.append(contents[0])\n",
    "            tags.append(contents[-1])\n",
    "                \n",
    "    return samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '.\\emerging_entities_17-master\\emerging_entities_17-master\\wnut17train.conll'\n",
    "train = read_file(path)\n",
    "\n",
    "path2 = '.\\emerging_entities_17-master\\emerging_entities_17-master\\emerging.test.conll'\n",
    "test = read_file(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(test)\n",
    "for i in range(length):\n",
    "    \n",
    "    l = len(test[i][1])\n",
    "    for j in range(l):\n",
    "        la = test[i][1][j].split(',')[0]\n",
    "        test[i][1][j]=la\n",
    "#test[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1a. Named Entity Recognition (NER) using CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features2(sent, i):\n",
    "    \n",
    "    word = sent[0][i]\n",
    "    #postag = sent[i][1]\n",
    "\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        #'postag': postag,\n",
    "        #'postag[:2]': postag[:2],\n",
    "    }\n",
    "    \n",
    "    if i > 0:\n",
    "        word1 = sent[0][i-1]\n",
    "        #postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            #'-1:postag': postag1,\n",
    "            #'-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features['BOS'] = True\n",
    "        \n",
    "    \n",
    "    if i < len(sent[0])-1:\n",
    "        word1 = sent[0][i+1]\n",
    "        #postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            #'+1:postag': postag1,\n",
    "            #'+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        # Features for words that are not at the end of a document\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features2(sent):\n",
    "    j=len(sent[0])\n",
    "    return [word2features2(sent, i) for i in range(j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels2(sent):\n",
    "    return sent[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence representations for sequence labelling\n",
    "X_train = [sent2features2(s) for s in train]\n",
    "y_train = [sent2labels2(s) for s in train]\n",
    "\n",
    "X_test = [sent2features2(s) for s in test]\n",
    "y_test = [sent2labels2(s) for s in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn_crfsuite in c:\\users\\administrator\\anaconda3\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: tabulate in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from sklearn_crfsuite) (0.8.7)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from sklearn_crfsuite) (0.9.7)\n",
      "Requirement already satisfied: six in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from sklearn_crfsuite) (1.15.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from sklearn_crfsuite) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "# train CRF model\n",
    "!pip install sklearn_crfsuite\n",
    "import sklearn_crfsuite\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.fit(X_train, y_train)\n",
    "# training model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['B-location',\n",
    " 'B-product',\n",
    " 'O',\n",
    " 'I-corporation',\n",
    " 'I-creative-work',\n",
    " 'I-person',\n",
    " 'B-person',\n",
    " 'B-corporation',\n",
    " 'I-group',\n",
    " 'I-product',\n",
    " 'B-creative-work',\n",
    " 'B-group',\n",
    " 'I-location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9164468422523492"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate CRF model\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect per-class results in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "              O      0.945     0.997     0.970     21934\n",
      "  B-corporation      0.000     0.000     0.000       117\n",
      "  I-corporation      0.000     0.000     0.000        24\n",
      "B-creative-work      0.455     0.022     0.041       231\n",
      "I-creative-work      0.474     0.038     0.070       237\n",
      "        B-group      0.200     0.031     0.054        97\n",
      "        I-group      0.267     0.103     0.148        39\n",
      "     B-location      0.294     0.164     0.211       122\n",
      "     I-location      0.300     0.154     0.203        39\n",
      "       B-person      0.500     0.101     0.169       355\n",
      "       I-person      0.462     0.178     0.257       101\n",
      "      B-product      0.000     0.000     0.000        56\n",
      "      I-product      0.000     0.000     0.000        42\n",
      "\n",
      "       accuracy                          0.939     23394\n",
      "      macro avg      0.300     0.137     0.163     23394\n",
      "   weighted avg      0.908     0.939     0.916     23394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1b. Named Entity Recognition (NER) using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels2(sent):\n",
    "    return sent[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2tokens2(sent):\n",
    "    return sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = [sent2tokens2(s) for s in train]\n",
    "y_train2 = [sent2labels2(s) for s in train]\n",
    "\n",
    "X_test2 = [sent2tokens2(s) for s in test]\n",
    "y_test2 = [sent2labels2(s) for s in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3394"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmjwjGB817dp",
    "outputId": "e7729f11-4621-4d42-89f3-6a2566dc8209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in training data: 13\n"
     ]
    }
   ],
   "source": [
    "# sentence representations for sequence labelling\n",
    "#train_sent_tokens = [sent2tokens(s) for s in train_sents]\n",
    "#train_labels = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "train_sent_tokens=X_train2\n",
    "\n",
    "train_labels = y_train2\n",
    "train_labels = [i for i in train_labels  if i != '']\n",
    "\n",
    "train_id_2_label = list(set([label for sent in train_labels for label in sent]))\n",
    "#train_id_2_label = [i for i in train_id_2_label  if i != '']\n",
    "train_label_2_id = {label:i for i, label in enumerate(train_id_2_label)}\n",
    "print(\"Number of unique labels in training data:\", len(train_id_2_label))\n",
    "\n",
    "def convert_labels_to_inds(sent_labels, label_2_id):\n",
    "    return [label_2_id[label] for label in sent_labels]\n",
    "\n",
    "train_label_inds = [convert_labels_to_inds(sent_labels, train_label_2_id) for sent_labels in train_labels]\n",
    "\n",
    "#test_sent_tokens = [sent2tokens(s) for s in test_sents]\n",
    "#test_labels = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "test_sent_tokens = X_test2\n",
    "test_labels =y_test2\n",
    "test_label_inds = [convert_labels_to_inds(s, train_label_2_id) for s in test_labels]\n",
    "test_id_2_label = list(set([label for sent in test_labels for label in sent]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "id": "m963KaxfbNbs"
   },
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "\n",
    "# converting tokenized sentence lists to vocabulary indices\n",
    "id_2_word = list(set([token for sent in train_sent_tokens for token in sent])) + [\"<pad>\", \"<unk>\"]\n",
    "word_2_id = {w:i for i,w in enumerate(id_2_word)}\n",
    "\n",
    "def convert_tokens_to_inds(sentence, word_2_id):\n",
    "    return [word_2_id.get(t, word_2_id[\"<unk>\"]) for t in sentence]\n",
    "\n",
    "# padding for windows\n",
    "def pad_sentence_for_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    return [pad_token]*window_size + sentence + [pad_token]*window_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "id": "v4Rcfpt-c25z"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "id": "NN7BXIZVueq5"
   },
   "outputs": [],
   "source": [
    "# Batching sentences together with a DataLoader\n",
    "\n",
    "def my_collate(data, window_size, word_2_id):\n",
    "    \"\"\"\n",
    "    For some chunk of sentences and labels\n",
    "        -add winow padding\n",
    "        -pad for lengths using pad_sequence\n",
    "        -convert our labels to one-hots\n",
    "        -return padded inputs, one-hot labels, and lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    x_s, y_s = zip(*data)\n",
    "\n",
    "    # deal with input sentences as we've seen\n",
    "    window_padded = [convert_tokens_to_inds(pad_sentence_for_window(sentence, window_size), word_2_id)\n",
    "                                                                                  for sentence in x_s]\n",
    "    # append zeros to each list of token ids in batch so that they are all the same length\n",
    "    padded = nn.utils.rnn.pad_sequence([torch.LongTensor(t) for t in window_padded], batch_first=True)\n",
    "    \n",
    "    # convert labels to one-hots\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for y in y_s:\n",
    "        lengths.append(len(y))\n",
    "        one_hot = torch.zeros(len(y), len(train_id_2_label))\n",
    "        y = torch.tensor(y)\n",
    "        y = y.unsqueeze(1)\n",
    "        label = one_hot.scatter_(1, y, 1)\n",
    "        labels.append(label)\n",
    "    padded_labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    return padded.long(), padded_labels, torch.LongTensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "id": "SeuW6LtGdiEr"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# Shuffle True is good practice for train loaders.\n",
    "# Use functools.partial to construct a partially populated collate function\n",
    "train_loader = DataLoader(list(zip(train_sent_tokens, train_label_inds)), \n",
    "                            batch_size=batch_size, shuffle=True, \n",
    "                            collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffgq3qmg2i6M",
    "outputId": "9e4aeb72-080f-49ee-f109-ea53c6597d39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor batched_input, batched_labels, batch_lengths in train_loader:\\n    pp.pprint((\"inputs\", batched_input, batched_input.size()))\\n    pp.pprint((\"labels\", batched_labels, batched_labels.size()))\\n    pp.pprint(batch_lengths)\\n    break\\n'"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for batched_input, batched_labels, batch_lengths in train_loader:\n",
    "    pp.pprint((\"inputs\", batched_input, batched_input.size()))\n",
    "    pp.pprint((\"labels\", batched_labels, batched_labels.size()))\n",
    "    pp.pprint(batch_lengths)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "id": "5sAza2qj-WKF"
   },
   "outputs": [],
   "source": [
    "class SoftmaxWordWindowClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A one-layer, binary word-window classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, vocab_size, pad_idx=0):\n",
    "        super(SoftmaxWordWindowClassifier, self).__init__()\n",
    "        \"\"\"\n",
    "        Instance variables.\n",
    "        \"\"\"\n",
    "        self.window_size = 2*config[\"half_window\"]+1\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.freeze_embeddings = config[\"freeze_embeddings\"]\n",
    "        \n",
    "        \"\"\"\n",
    "        Embedding layer\n",
    "        -model holds an embedding for each layer in our vocab\n",
    "        -sets aside a special index in the embedding matrix for padding vector (of zeros)\n",
    "        -by default, embeddings are parameters (so gradients pass through them)\n",
    "        \"\"\"\n",
    "        self.embed_layer = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_idx)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embed_layer.weight.requires_grad = False\n",
    "        \n",
    "        \"\"\"\n",
    "        Hidden layer\n",
    "        -we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n",
    "        -nn.Sequential allows you to efficiently specify sequentially structured models\n",
    "            -first the linear transformation is evoked on the embedded word windows\n",
    "            -next the nonlinear transformation tanh is evoked.\n",
    "        \"\"\"\n",
    "        self.hidden_layer = nn.Sequential(nn.Linear(self.window_size*self.embed_dim, \n",
    "                                                    self.hidden_dim), \n",
    "                                          nn.Tanh())\n",
    "        \n",
    "        \"\"\"\n",
    "        Output layer\n",
    "        -we want to map elements of the output layer (of size self.hidden dim) to a number of classes.\n",
    "        \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "        \n",
    "        \"\"\"\n",
    "        Softmax\n",
    "        -The final step of the softmax classifier: mapping final hidden layer to class scores.\n",
    "        -pytorch has both logsoftmax and softmax functions (and many others)\n",
    "        -since our loss is the negative LOG likelihood, we use logsoftmax\n",
    "        -technically you can take the softmax, and take the log but PyTorch's implementation\n",
    "         is optimized to avoid numerical underflow issues.\n",
    "        \"\"\"\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Let B:= batch_size\n",
    "            L:= window-padded sentence length\n",
    "            D:= self.embed_dim\n",
    "            S:= self.window_size\n",
    "            H:= self.hidden_dim\n",
    "            \n",
    "        inputs: a (B, L) tensor of token indices\n",
    "        \"\"\"\n",
    "        B, L = inputs.size()\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L) LongTensor\n",
    "        Outputs a (B, L~, S) LongTensor\n",
    "        \"\"\"\n",
    "        # Fist, get our word windows for each word in our input.\n",
    "        token_windows = inputs.unfold(1, self.window_size, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "        \n",
    "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "        assert token_windows.size() == (B, adjusted_length, self.window_size)\n",
    "        \n",
    "        \"\"\"\n",
    "        Embedding.\n",
    "        Takes in a torch.LongTensor of size (B, L~, S) \n",
    "        Outputs a (B, L~, S, D) FloatTensor.\n",
    "        \"\"\"\n",
    "        embedded_windows = self.embed_layer(token_windows)\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L~, S, D) FloatTensor.\n",
    "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "        \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Layer 1.\n",
    "        Takes in a (B, L~, S*D) FloatTensor.\n",
    "        Resizes it into a (B, L~, H) FloatTensor\n",
    "        \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "        \n",
    "        \"\"\"\n",
    "        Layer 2\n",
    "        Takes in a (B, L~, H) FloatTensor.\n",
    "        Resizes it into a (B, L~, 2) FloatTensor.\n",
    "        \"\"\"\n",
    "        output = self.output_layer(layer_1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Softmax.\n",
    "        Takes in a (B, L~, 2) FloatTensor of unnormalized class scores.\n",
    "        Outputs a (B, L~, 2) FloatTensor of (log-)normalized class scores.\n",
    "        \"\"\"\n",
    "        output = self.log_softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "id": "6kZIRM_I-gNI"
   },
   "outputs": [],
   "source": [
    "def loss_function(outputs, labels, lengths):\n",
    "    \"\"\"Computes negative LL loss on a batch of model predictions.\"\"\"\n",
    "    B, L, num_classes = outputs.size()\n",
    "    num_elems = lengths.sum().float()\n",
    "        \n",
    "    # get only the values with non-zero labels\n",
    "    loss = outputs*labels\n",
    "    \n",
    "    # rescale average\n",
    "    return -loss.sum() / num_elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "id": "GyATHYB5-5nz"
   },
   "outputs": [],
   "source": [
    "def train_epoch(loss_function, optimizer, model, train_data):\n",
    "    \n",
    "    ## For each batch, we must reset the gradients\n",
    "    ## stored by the model.   \n",
    "    total_loss = 0\n",
    "    for batch, labels, lengths in train_data:\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # evoke model in training mode on batch\n",
    "        outputs = model.forward(batch)\n",
    "        # compute loss w.r.t batch\n",
    "        loss = loss_function(outputs, labels, lengths)\n",
    "        # pass gradients back, startiing on loss value\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # return the total to keep track of how you did this time around\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "id": "AdCbC3er-9GM"
   },
   "outputs": [],
   "source": [
    "config = {\"batch_size\": 4,\n",
    "          \"half_window\": 2,\n",
    "          \"embed_dim\": 25,\n",
    "          \"hidden_dim\": 25,\n",
    "          \"num_classes\": 13,\n",
    "          \"freeze_embeddings\": False,\n",
    "         }\n",
    "learning_rate = .0002\n",
    "num_epochs = 5\n",
    "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUIPUmdJ_R9U",
    "outputId": "d84068e7-0524-42ce-a90e-b879d84a08b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2019.0639123916626\n",
      "1 1827.551206946373\n",
      "2 1640.8150030374527\n",
      "3 1462.7690185308456\n",
      "4 1294.7638195753098\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
    "    print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1rTPZHVRrpz"
   },
   "source": [
    "### Evaluation\n",
    "There is much more O entities in data set, but we’re more interested in other entities. To account for this we’ll use averaged F1 score computed for all labels except for O. sklearn-crfsuite.metrics package provides some useful metrics for sequence classification task, including this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "id": "Ei4N1XO9czHO"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(list(zip(test_sent_tokens, test_label_inds)), \n",
    "                            batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "id": "QMoQKT9Lc_-H"
   },
   "outputs": [],
   "source": [
    "test_outputs = []\n",
    "for test_instance, labs, _ in test_loader:\n",
    "    outputs_full = model.forward(test_instance)\n",
    "    outputs = torch.argmax(outputs_full, dim=2)\n",
    "    for i in range(outputs.size(0)):\n",
    "        test_outputs.append(outputs[i].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "id": "hN-RvWGZ25xJ"
   },
   "outputs": [],
   "source": [
    "y_test = test_labels\n",
    "y_pred = []\n",
    "for test, pred in zip(test_labels, test_outputs):\n",
    "    y_pred.append([train_id_2_label[id] for id in pred[:len(test)]])\n",
    "\n",
    "assert len(y_pred) == len(y_test), '{} vs. {}'.format(len(y_pred), len(y_test))\n",
    "for i, pred, test in zip(list(range(len(y_pred))), y_pred, y_test):\n",
    "    assert len(pred) == len(test), '{}: {} vs. {}'.format(i, len(pred), len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdTxgk0Z3EX4",
    "outputId": "3b18e05a-4df5-41c4-f612-605591a8b09c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\administrator\\anaconda3\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: six in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (1.15.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (4.47.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (0.9.7)\n",
      "Requirement already satisfied: tabulate in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (0.8.7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8977781306441406"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate Softmax model\n",
    "!pip install sklearn-crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=test_id_2_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVNbvrgrRrp6",
    "outputId": "3ad6698f-bdad-4e9f-a319-e4e3f0535b54"
   },
   "outputs": [],
   "source": [
    "#print(y_pred[223:1582])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thb1I8SRRrp9"
   },
   "source": [
    "### Inspect per-class results in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9wfAp8iRrp9",
    "outputId": "89eb4db9-a38f-4bd1-d6b1-37517e7d60fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "              O      0.937     0.978     0.957     21934\n",
      "  B-corporation      0.000     0.000     0.000       117\n",
      "  I-corporation      0.000     0.000     0.000        24\n",
      "B-creative-work      0.000     0.000     0.000       231\n",
      "I-creative-work      0.000     0.000     0.000       237\n",
      "        B-group      0.008     0.010     0.009        97\n",
      "        I-group      0.000     0.000     0.000        39\n",
      "     B-location      0.000     0.000     0.000       122\n",
      "     I-location      0.000     0.000     0.000        39\n",
      "       B-person      0.042     0.003     0.005       355\n",
      "       I-person      0.000     0.000     0.000       101\n",
      "      B-product      0.000     0.000     0.000        56\n",
      "      I-product      0.000     0.000     0.000        42\n",
      "\n",
      "       accuracy                          0.917     23394\n",
      "      macro avg      0.076     0.076     0.075     23394\n",
      "   weighted avg      0.880     0.917     0.898     23394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    test_id_2_label,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1c. alternative values of the hyper-parameters using Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning hyper-parameters 'batch size':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\": 8,\n",
    "          \"half_window\": 2,\n",
    "          \"embed_dim\": 25,\n",
    "          \"hidden_dim\": 25,\n",
    "          \"num_classes\": 13,\n",
    "          \"freeze_embeddings\": False,\n",
    "         }\n",
    "learning_rate = .0002\n",
    "num_epochs = 5\n",
    "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2042.8751380443573\n",
      "1 1879.1225023269653\n",
      "2 1718.3085926771164\n",
      "3 1560.2284051179886\n",
      "4 1405.0115276575089\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
    "    print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(list(zip(test_sent_tokens, test_label_inds)), \n",
    "                            batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = []\n",
    "for test_instance, labs, _ in test_loader:\n",
    "    outputs_full = model.forward(test_instance)\n",
    "    outputs = torch.argmax(outputs_full, dim=2)\n",
    "    for i in range(outputs.size(0)):\n",
    "        test_outputs.append(outputs[i].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_labels\n",
    "y_pred = []\n",
    "for test, pred in zip(test_labels, test_outputs):\n",
    "    y_pred.append([train_id_2_label[id] for id in pred[:len(test)]])\n",
    "\n",
    "assert len(y_pred) == len(y_test), '{} vs. {}'.format(len(y_pred), len(y_test))\n",
    "for i, pred, test in zip(list(range(len(y_pred))), y_pred, y_test):\n",
    "    assert len(pred) == len(test), '{}: {} vs. {}'.format(i, len(pred), len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.898585932490903"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate Softmax model\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=test_id_2_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning hyper-parameters 'embedding size':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\": 4,\n",
    "          \"half_window\": 2,\n",
    "          \"embed_dim\": 50,\n",
    "          \"hidden_dim\": 25,\n",
    "          \"num_classes\": 13,\n",
    "          \"freeze_embeddings\":False,\n",
    "         }\n",
    "learning_rate = .0002\n",
    "num_epochs = 5\n",
    "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2123.584525823593\n",
      "1 1916.9141519069672\n",
      "2 1709.5205612182617\n",
      "3 1503.984883069992\n",
      "4 1309.1455084085464\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
    "    print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(list(zip(test_sent_tokens, test_label_inds)), \n",
    "                            batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = []\n",
    "for test_instance, labs, _ in test_loader:\n",
    "    outputs_full = model.forward(test_instance)\n",
    "    outputs = torch.argmax(outputs_full, dim=2)\n",
    "    for i in range(outputs.size(0)):\n",
    "        test_outputs.append(outputs[i].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_labels\n",
    "y_pred = []\n",
    "for test, pred in zip(test_labels, test_outputs):\n",
    "    y_pred.append([train_id_2_label[id] for id in pred[:len(test)]])\n",
    "\n",
    "assert len(y_pred) == len(y_test), '{} vs. {}'.format(len(y_pred), len(y_test))\n",
    "for i, pred, test in zip(list(range(len(y_pred))), y_pred, y_test):\n",
    "    assert len(pred) == len(test), '{}: {} vs. {}'.format(i, len(pred), len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8776186523574464"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate Softmax model\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=test_id_2_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning hyper-parameters 'hidden layer size':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\": 4,\n",
    "          \"half_window\": 2,\n",
    "          \"embed_dim\": 25,\n",
    "          \"hidden_dim\": 50,\n",
    "          \"num_classes\": 13,\n",
    "          \"freeze_embeddings\":False,\n",
    "         }\n",
    "learning_rate = .0002\n",
    "num_epochs = 5\n",
    "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1989.6052763462067\n",
      "1 1812.6522456407547\n",
      "2 1637.9923903942108\n",
      "3 1466.5012859106064\n",
      "4 1301.9076025485992\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
    "    print(epoch, epoch_loss)\n",
    "    \n",
    "test_loader = DataLoader(list(zip(test_sent_tokens, test_label_inds)), \n",
    "                            batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))\n",
    "\n",
    "test_outputs = []\n",
    "for test_instance, labs, _ in test_loader:\n",
    "    outputs_full = model.forward(test_instance)\n",
    "    outputs = torch.argmax(outputs_full, dim=2)\n",
    "    for i in range(outputs.size(0)):\n",
    "        test_outputs.append(outputs[i].tolist())\n",
    "\n",
    "y_test = test_labels\n",
    "y_pred = []\n",
    "for test, pred in zip(test_labels, test_outputs):\n",
    "    y_pred.append([train_id_2_label[id] for id in pred[:len(test)]])\n",
    "\n",
    "assert len(y_pred) == len(y_test), '{} vs. {}'.format(len(y_pred), len(y_test))\n",
    "for i, pred, test in zip(list(range(len(y_pred))), y_pred, y_test):\n",
    "    assert len(pred) == len(test), '{}: {} vs. {}'.format(i, len(pred), len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986037288753662"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate Softmax model\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=test_id_2_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning hyper-parameters 'Freeze embeddings or not':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\": 4,\n",
    "          \"half_window\": 2,\n",
    "          \"embed_dim\": 25,\n",
    "          \"hidden_dim\": 25,\n",
    "          \"num_classes\": 13,\n",
    "          \"freeze_embeddings\":True,\n",
    "         }\n",
    "learning_rate = .0002\n",
    "num_epochs = 5\n",
    "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2073.004519701004\n",
      "1 1899.479402065277\n",
      "2 1730.0330951213837\n",
      "3 1564.0777504444122\n",
      "4 1400.8539677858353\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
    "    print(epoch, epoch_loss)\n",
    "    \n",
    "test_loader = DataLoader(list(zip(test_sent_tokens, test_label_inds)), \n",
    "                            batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))\n",
    "\n",
    "test_outputs = []\n",
    "for test_instance, labs, _ in test_loader:\n",
    "    outputs_full = model.forward(test_instance)\n",
    "    outputs = torch.argmax(outputs_full, dim=2)\n",
    "    for i in range(outputs.size(0)):\n",
    "        test_outputs.append(outputs[i].tolist())\n",
    "\n",
    "y_test = test_labels\n",
    "y_pred = []\n",
    "for test, pred in zip(test_labels, test_outputs):\n",
    "    y_pred.append([train_id_2_label[id] for id in pred[:len(test)]])\n",
    "\n",
    "assert len(y_pred) == len(y_test), '{} vs. {}'.format(len(y_pred), len(y_test))\n",
    "for i, pred, test in zip(list(range(len(y_pred))), y_pred, y_test):\n",
    "    assert len(pred) == len(test), '{}: {} vs. {}'.format(i, len(pred), len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867768645400548"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate Softmax model\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=test_id_2_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning hyper-parameters 'learning rate':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\": 4,\n",
    "          \"half_window\": 2,\n",
    "          \"embed_dim\": 25,\n",
    "          \"hidden_dim\": 25,\n",
    "          \"num_classes\": 13,\n",
    "          \"freeze_embeddings\":False,\n",
    "         }\n",
    "learning_rate = .0005\n",
    "num_epochs = 5\n",
    "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1982.3485270738602\n",
      "1 1515.8935358524323\n",
      "2 1111.8003361225128\n",
      "3 822.4663741588593\n",
      "4 639.1291683316231\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
    "    print(epoch, epoch_loss)\n",
    "    \n",
    "test_loader = DataLoader(list(zip(test_sent_tokens, test_label_inds)), \n",
    "                            batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))\n",
    "\n",
    "test_outputs = []\n",
    "for test_instance, labs, _ in test_loader:\n",
    "    outputs_full = model.forward(test_instance)\n",
    "    outputs = torch.argmax(outputs_full, dim=2)\n",
    "    for i in range(outputs.size(0)):\n",
    "        test_outputs.append(outputs[i].tolist())\n",
    "\n",
    "y_test = test_labels\n",
    "y_pred = []\n",
    "for test, pred in zip(test_labels, test_outputs):\n",
    "    y_pred.append([train_id_2_label[id] for id in pred[:len(test)]])\n",
    "\n",
    "assert len(y_pred) == len(y_test), '{} vs. {}'.format(len(y_pred), len(y_test))\n",
    "for i, pred, test in zip(list(range(len(y_pred))), y_pred, y_test):\n",
    "    assert len(pred) == len(test), '{}: {} vs. {}'.format(i, len(pred), len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9045238301676479"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate Softmax model\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=test_id_2_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI6127-2021Spring-week03-tutorial-softmax-classifier-for-NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
